{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_raw = pd.read_csv('data/oil.csv', parse_dates=[\"date\"], index_col=\"date\")\n",
    "oil = pd.DataFrame()\n",
    "oil[\"date\"] = np.arange(oil_raw.index.min(), oil_raw.index.max() + np.timedelta64(7, \"D\"), np.timedelta64(1, \"D\"), dtype=\"datetime64[D]\")\n",
    "oil = oil.join(oil_raw, on=\"date\", how=\"left\").bfill().ffill()\n",
    "oil.date = oil.date.astype(\"datetime64[ns]\")\n",
    "oil.to_csv(\"gen_data/oil.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = pd.read_csv(\"data/stores.csv\")\n",
    "holidays = pd.read_csv(\"data/holidays_events.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "eq = holidays.description.str.startswith(\"Terremoto Manabi\")\n",
    "eq_days = holidays[eq]\n",
    "holidays = holidays[~eq]\n",
    "\n",
    "holidays\n",
    "stores = stores[[\"store_nbr\", \"city\", \"state\"]]\n",
    "\n",
    "r_holidays = stores.merge(holidays[holidays.locale == \"Regional\"], left_on=\"state\", right_on=\"locale_name\", how=\"inner\")[[\"store_nbr\", \"date\", \"locale\"]]\n",
    "l_holidays = stores.merge(holidays[holidays.locale == \"Local\"], left_on=\"city\", right_on=\"locale_name\", how=\"inner\")[[\"store_nbr\", \"date\", \"locale\"]]\n",
    "n_holidays = stores.merge(holidays[holidays.locale == \"National\"], how=\"cross\")[[\"store_nbr\", \"date\", \"locale\"]]\n",
    "eq_days[\"locale\"] = \"Earthquake\"\n",
    "eq_holidays = stores.merge(eq_days, how=\"cross\")[[\"store_nbr\", \"date\", \"locale\"]]\n",
    "\n",
    "holidays = pd.concat([r_holidays, l_holidays, n_holidays, eq_holidays])\n",
    "holidays = holidays.drop_duplicates()\n",
    "holidays = pd.get_dummies(holidays, columns=[\"locale\"])\n",
    "holidays = holidays.groupby([\"store_nbr\", \"date\"]).sum().reset_index()\n",
    "holidays = holidays.sort_values([\"date\"])\n",
    "\n",
    "holidays.to_csv(\"gen_data/holidays.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Uber Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54 entries, 0 to 53\n",
      "Data columns (total 23 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   store_nbr         54 non-null     int64\n",
      " 1   store_type_A      54 non-null     uint8\n",
      " 2   store_type_B      54 non-null     uint8\n",
      " 3   store_type_C      54 non-null     uint8\n",
      " 4   store_type_D      54 non-null     uint8\n",
      " 5   store_type_E      54 non-null     uint8\n",
      " 6   store_cluster_1   54 non-null     uint8\n",
      " 7   store_cluster_2   54 non-null     uint8\n",
      " 8   store_cluster_3   54 non-null     uint8\n",
      " 9   store_cluster_4   54 non-null     uint8\n",
      " 10  store_cluster_5   54 non-null     uint8\n",
      " 11  store_cluster_6   54 non-null     uint8\n",
      " 12  store_cluster_7   54 non-null     uint8\n",
      " 13  store_cluster_8   54 non-null     uint8\n",
      " 14  store_cluster_9   54 non-null     uint8\n",
      " 15  store_cluster_10  54 non-null     uint8\n",
      " 16  store_cluster_11  54 non-null     uint8\n",
      " 17  store_cluster_12  54 non-null     uint8\n",
      " 18  store_cluster_13  54 non-null     uint8\n",
      " 19  store_cluster_14  54 non-null     uint8\n",
      " 20  store_cluster_15  54 non-null     uint8\n",
      " 21  store_cluster_16  54 non-null     uint8\n",
      " 22  store_cluster_17  54 non-null     uint8\n",
      "dtypes: int64(1), uint8(22)\n",
      "memory usage: 1.7 KB\n"
     ]
    }
   ],
   "source": [
    "stores = pd.read_csv(\"data/stores.csv\")\n",
    "stores = stores[[\"store_nbr\", \"type\", \"cluster\"]].rename(columns={\"type\": \"store_type\", \"cluster\": \"store_cluster\"})\n",
    "stores = pd.get_dummies(stores, columns=[\"store_type\", \"store_cluster\"])\n",
    "dummies = list(set(stores.columns) - {\"store_nbr\"})\n",
    "stores[dummies] = stores[dummies].astype(\"uint8\")\n",
    "stores.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1703 entries, 0 to 1702\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   date        1703 non-null   datetime64[ns]\n",
      " 1   dcoilwtico  1703 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(1)\n",
      "memory usage: 26.7 KB\n"
     ]
    }
   ],
   "source": [
    "oil = pd.read_csv(\"gen_data/oil.csv\", parse_dates=[\"date\"])\n",
    "oil.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9601 entries, 0 to 9600\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   store_nbr          9601 non-null   int64         \n",
      " 1   date               9601 non-null   datetime64[ns]\n",
      " 2   locale_Earthquake  9601 non-null   uint8         \n",
      " 3   locale_Local       9601 non-null   uint8         \n",
      " 4   locale_National    9601 non-null   uint8         \n",
      " 5   locale_Regional    9601 non-null   uint8         \n",
      "dtypes: datetime64[ns](1), int64(1), uint8(4)\n",
      "memory usage: 187.6 KB\n"
     ]
    }
   ],
   "source": [
    "holidays = pd.read_csv(\"gen_data/holidays.csv\", parse_dates=[\"date\"])\n",
    "locale = [c for c in holidays.columns if c.startswith(\"locale\")]\n",
    "holidays[locale] = holidays[locale].fillna(0).astype(\"uint8\")\n",
    "holidays.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug(df):\n",
    "    df = df.merge(stores, left_on=\"store_nbr\", right_on=\"store_nbr\", how=\"inner\")\n",
    "    df = df.merge(oil, left_on=\"date\", right_on=\"date\", how=\"inner\")\n",
    "    df = df.merge(holidays, left_on=[\"date\", \"store_nbr\"], right_on=[\"date\", \"store_nbr\"], how=\"left\")\n",
    "    df[locale] = df[locale].fillna(0).astype(\"uint8\")\n",
    "\n",
    "    df[\"month\"] = df.date.dt.month.astype(\"uint8\")\n",
    "    df[\"year\"] = df.date.dt.year.astype(\"uint16\")\n",
    "    df[\"doy\"] = df.date.dt.dayofyear.astype(\"uint16\")\n",
    "    df[\"dow\"] = df.date.dt.dayofweek.astype(\"uint8\")\n",
    "    df = df.drop(columns=[\"date\"])\n",
    "\n",
    "    df = pd.get_dummies(df, columns=[\"family\", \"dow\", \"month\", \"store_nbr\"])\n",
    "    d = [c for c in df.columns if c.startswith(\"dow_\") or c.startswith(\"month_\") or c.startswith(\"family_\") or c.startswith(\"store_nbr_\")]\n",
    "    df[d] = df[d].astype(\"uint8\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', parse_dates=[\"date\"])\n",
    "new_train = aug(train)\n",
    "new_train.to_csv(\"gen_data/train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/zjyw1l0d37x3lbbq6x4vvs_00000gn/T/ipykernel_54057/1145219075.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_test[col] = 0\n",
      "/var/folders/l_/zjyw1l0d37x3lbbq6x4vvs_00000gn/T/ipykernel_54057/1145219075.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_test[col] = 0\n",
      "/var/folders/l_/zjyw1l0d37x3lbbq6x4vvs_00000gn/T/ipykernel_54057/1145219075.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_test[col] = 0\n",
      "/var/folders/l_/zjyw1l0d37x3lbbq6x4vvs_00000gn/T/ipykernel_54057/1145219075.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_test[col] = 0\n",
      "/var/folders/l_/zjyw1l0d37x3lbbq6x4vvs_00000gn/T/ipykernel_54057/1145219075.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_test[col] = 0\n",
      "/var/folders/l_/zjyw1l0d37x3lbbq6x4vvs_00000gn/T/ipykernel_54057/1145219075.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_test[col] = 0\n",
      "/var/folders/l_/zjyw1l0d37x3lbbq6x4vvs_00000gn/T/ipykernel_54057/1145219075.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_test[col] = 0\n",
      "/var/folders/l_/zjyw1l0d37x3lbbq6x4vvs_00000gn/T/ipykernel_54057/1145219075.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_test[col] = 0\n",
      "/var/folders/l_/zjyw1l0d37x3lbbq6x4vvs_00000gn/T/ipykernel_54057/1145219075.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_test[col] = 0\n",
      "/var/folders/l_/zjyw1l0d37x3lbbq6x4vvs_00000gn/T/ipykernel_54057/1145219075.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_test[col] = 0\n",
      "/var/folders/l_/zjyw1l0d37x3lbbq6x4vvs_00000gn/T/ipykernel_54057/1145219075.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  new_test[col] = 0\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('data/test.csv', parse_dates=[\"date\"])\n",
    "new_test = aug(test)\n",
    "for col in new_train.columns:\n",
    "    if col not in new_test.columns:\n",
    "        new_test[col] = 0\n",
    "new_test = new_test[new_train.columns]\n",
    "new_test = new_test.drop(columns=[\"sales\"])\n",
    "new_test.to_csv(\"gen_data/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
